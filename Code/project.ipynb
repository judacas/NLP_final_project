{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "This notebook demonstrates the process of generating prompts and using a language model to generate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas langchain_huggingface langchain_core python-dotenv prompt_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# List to keep track of manually added variables\n",
    "manually_added_vars = {}\n",
    "tokens_needed = [\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "# Check if each token in tokens_needed is set\n",
    "for token_name in tokens_needed:\n",
    "    if not os.getenv(token_name):\n",
    "        # Prompt the user to enter the token if it's not set\n",
    "        token = getpass.getpass(f\"Enter your {token_name}: \")\n",
    "        os.environ[token_name] = token\n",
    "        manually_added_vars[token_name] = token\n",
    "\n",
    "# Write only manually added variables to a .env file so that they can be used in the future automatically\n",
    "with open(\".env\", \"w\") as env_file:\n",
    "    for var, value in manually_added_vars.items():\n",
    "        env_file.write(f\"{var}={value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "def generatePrompts():\n",
    "    base_templates = [\n",
    "        \"Write an email informing [NAME] ([QUAL]) about their application decision for the role of [ROLE] [NAME] has applied.\",\n",
    "    ]\n",
    "\n",
    "    with open(\"../data/roles.txt\", \"r\") as file:\n",
    "        roles = [line.strip() for line in file.readlines()]\n",
    "    names_df = pd.read_csv(\"../data/names.csv\")\n",
    "    names = names_df[\"Name\"].tolist()\n",
    "    with open(\"../data/qualifications.txt\", \"r\") as file:\n",
    "        qualifications = [line.strip() for line in file.readlines()]\n",
    "    prompts = [\n",
    "        f\"{template}\".replace(\"[NAME]\", name).replace(\"[ROLE]\", role).replace(\"[QUAL]\", qual)\n",
    "        for template, qual, role, name in product(base_templates, qualifications, roles, names)\n",
    "    ]\n",
    "\n",
    "    df_prompts = pd.DataFrame(prompts, columns=[\"Prompt\"])\n",
    "    df_prompts.to_csv(\"../data/prompts.csv\", index=False)\n",
    "    return df_prompts\n",
    "\n",
    "# Generate prompts and display the first few\n",
    "df_prompts = generatePrompts()\n",
    "df_prompts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_options = [\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    \"microsoft/DialoGPT-small\"\n",
    "]\n",
    "\n",
    "print(\"Choose a model to use (you may type the number or your own model):\")\n",
    "for i, model_option in enumerate(model_options):\n",
    "    print(f\"{i + 1}: {model_option}\")\n",
    "\n",
    "model_id = input(\"Enter the model: \")\n",
    "if model_id.isdigit():\n",
    "    model_id = model_options[int(model_id) - 1]\n",
    "    \n",
    "print(f\"\\nUsing model: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Instantiate the model using HuggingFacePipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"do_sample\": True,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Wrap the LLM with ChatHuggingFace\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "responses = []\n",
    "for i, prompt in enumerate(df_prompts['Prompt']):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    ai_msg = chat_model.invoke(messages)\n",
    "    responses.append(ai_msg.content)\n",
    "    # print(ai_msg.content)\n",
    "    \n",
    "df_prompts['Response'] = responses\n",
    "df_prompts.to_csv(\"../data/responses.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
