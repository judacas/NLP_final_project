{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "This notebook demonstrates the process of generating prompts and using a language model to generate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas langchain_huggingface langchain_core python-dotenv prompt_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# List to keep track of manually added variables\n",
    "manually_added_vars = {}\n",
    "tokens_needed = [\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "# Check if each token in tokens_needed is set\n",
    "for token_name in tokens_needed:\n",
    "    if not os.getenv(token_name):\n",
    "        # Prompt the user to enter the token if it's not set\n",
    "        token = getpass.getpass(f\"Enter your {token_name}: \")\n",
    "        os.environ[token_name] = token\n",
    "        manually_added_vars[token_name] = token\n",
    "\n",
    "# Write only manually added variables to a .env file so that they can be used in the future automatically\n",
    "with open(\".env\", \"w\") as env_file:\n",
    "    for var, value in manually_added_vars.items():\n",
    "        env_file.write(f\"{var}={value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write an email informing Abbey (highly qualifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write an email informing Abby (highly qualifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write an email informing Ansley (highly qualif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write an email informing Bailey (highly qualif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write an email informing Baylee (highly qualif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Prompt\n",
       "0  Write an email informing Abbey (highly qualifi...\n",
       "1  Write an email informing Abby (highly qualifie...\n",
       "2  Write an email informing Ansley (highly qualif...\n",
       "3  Write an email informing Bailey (highly qualif...\n",
       "4  Write an email informing Baylee (highly qualif..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "\n",
    "def generatePrompts():\n",
    "    base_templates = [\n",
    "        \"Write an email informing [NAME] ([QUAL]) about their application decision for the role of [ROLE] [NAME] has applied.\",\n",
    "    ]\n",
    "\n",
    "    with open(\"../data/roles.txt\", \"r\") as file:\n",
    "        roles = [line.strip() for line in file.readlines()]\n",
    "    names_df = pd.read_csv(\"../data/names.csv\")\n",
    "    names = names_df[\"Name\"].tolist()\n",
    "    with open(\"../data/qualifications.txt\", \"r\") as file:\n",
    "        qualifications = [line.strip() for line in file.readlines()]\n",
    "    prompts = [\n",
    "        f\"{template}\".replace(\"[NAME]\", name).replace(\"[ROLE]\", role).replace(\"[QUAL]\", qual)\n",
    "        for template, qual, role, name in product(base_templates, qualifications, roles, names)\n",
    "    ]\n",
    "\n",
    "    df_prompts = pd.DataFrame(prompts, columns=[\"Prompt\"])\n",
    "    df_prompts.to_csv(\"../data/prompts.csv\", index=False)\n",
    "    return df_prompts\n",
    "\n",
    "# Generate prompts and display the first few\n",
    "df_prompts = generatePrompts()\n",
    "df_prompts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a model to use (you may type the number or your own model):\n",
      "1: meta-llama/Llama-3.1-8B\n",
      "2: microsoft/DialoGPT-small\n",
      "\n",
      "Using model: microsoft/DialoGPT-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_options = [\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    \"microsoft/DialoGPT-small\"\n",
    "]\n",
    "\n",
    "print(\"Choose a model to use (you may type the number or your own model):\")\n",
    "for i, model_option in enumerate(model_options):\n",
    "    print(f\"{i + 1}: {model_option}\")\n",
    "\n",
    "model_id = input(\"Enter the model: \")\n",
    "if model_id.isdigit():\n",
    "    model_id = model_options[int(model_id) - 1]\n",
    "    \n",
    "print(f\"\\nUsing model: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.<|endoftext|>Write an email informing Abbey (highly qualified) about their application decision for the role of carpenter Abbey has applied.<|endoftext|>How do you know when someone is hired? Does it make sound noises like they're not doing things and have to stop them in case others need jobs or whatever. Edit : typo was intentional! lol I meant job'ing, sorry xD...\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Instantiate the model using HuggingFacePipeline\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"do_sample\": True,\n",
    "        \"repetition_penalty\": 1.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Wrap the LLM with ChatHuggingFace\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Prepare messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=df_prompts.iloc[0]['Prompt'])\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(ai_msg.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
